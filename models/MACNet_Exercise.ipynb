{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MACNet Exercise.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alainray/vision/blob/master/models/MACNet_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Crm6qXOCH44Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MACNet: \n",
        "Implementation based on the paper \"Compositional Attention Networks for Machine Reasoning,  Drew A. Hudson, Christopher D. Manning\"  https://arxiv.org/pdf/1803.03067.pdf"
      ]
    },
    {
      "metadata": {
        "id": "3fwWIe9yH5MH",
        "colab_type": "code",
        "outputId": "7846de90-0e9d-48c9-ab21-64afc3fbcd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n",
        "!pip install -U pillow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python2.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python2.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python2.7/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already up-to-date: pillow in /usr/local/lib/python2.7/dist-packages (5.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k-mdHQ9gvpqR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "3ae351c6-b3f5-450f-b82a-dd8b99457bb7"
      },
      "cell_type": "code",
      "source": [
        "!curl -L -o 'sample.zip' 'https://www.dropbox.com/s/zpubau7qezrwfx4/dogs_cats_sample.zip?dl=0'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  1204    0  1204    0     0    740      0 --:--:--  0:00:01 --:--:--     0\n",
            "100 4357k  100 4357k    0     0  1364k      0  0:00:03  0:00:03 --:--:-- 5669k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IO9O4_40vozz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip sample.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0E-6k3gUpE64",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch\n",
        "\n",
        "class MACNet(nn.Module):\n",
        "  def __init__(self, embed_dim, hidden_dim, reasoning_steps=9,n_classes=2):\n",
        "    super(MACNet, self).__init__()\n",
        "    \n",
        "    #Net to process inputs\n",
        "    self.inputLSTM=nn.LSTM(embed_dim, hidden_dim, bidirectional=True)\n",
        "    self.image_processor=self.load_resnet_model(hidden_dim)\n",
        "    \n",
        "    #Net defining params\n",
        "    self.p=reasoning_steps\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.embed_dim=embed_dim\n",
        "    \n",
        "    #Parameters\n",
        "    #Input Unit\n",
        "    self.q=torch.zeros(2*hidden_dim)\n",
        "    self.q_i=torch.zeros(self.p,hidden_dim)\n",
        "    self.q_bias=nn.Parameter(torch.randn(self.p, hidden_dim))\n",
        "    self.cq_i=None\n",
        "    self.knowledge_base=None\n",
        "    self.contextual_words=None\n",
        "    #Control Unit\n",
        "    self.W_CQ=nn.Parameter(torch.randn(hidden_dim,2*hidden_dim))\n",
        "    self.W_CA=nn.Parameter(torch.randn(1,hidden_dim))\n",
        "    self.bias_ca=nn.Parameter(torch.randn(self.p, 1))\n",
        "    #Read Unit\n",
        "    self.W_M=nn.Parameter(torch.randn(hidden_dim,hidden_dim))\n",
        "    self.W_K=nn.Parameter(torch.randn(hidden_dim,hidden_dim))\n",
        "    self.bias_m=nn.Parameter(torch.randn(hidden_dim,1))\n",
        "    self.bias_k=nn.Parameter(torch.randn(hidden_dim,1))\n",
        "    self.bias_ihw=nn.Parameter(torch.randn(hidden_dim,1))\n",
        "    self.bias_ra=nn.Parameter(torch.randn(1,1))\n",
        "    self.W_IHW=nn.Parameter(torch.randn(hidden_dim,2*hidden_dim))\n",
        "    self.W_R=nn.Parameter(torch.randn(1,hidden_dim))\n",
        "    #Write Unit\n",
        "    self.W_S=nn.Parameter(torch.randn(hidden_dim,hidden_dim))\n",
        "    self.W_P=nn.Parameter(torch.randn(hidden_dim,hidden_dim))\n",
        "    self.W_CI=nn.Parameter(torch.randn(1,hidden_dim))\n",
        "    self.W_MI=nn.Parameter(torch.randn(hidden_dim,2*hidden_dim))\n",
        "    self.W_SA=nn.Parameter(torch.randn(1,hidden_dim))\n",
        "    self.bias_mi=nn.Parameter(torch.randn(hidden_dim,1))\n",
        "    self.bias_mip=nn.Parameter(torch.randn(hidden_dim,1))\n",
        "    self.bias_ci=nn.Parameter(torch.randn(1))\n",
        "    self.bias_sa=nn.Parameter(torch.randn(1))\n",
        "    #Output Unit\n",
        "    self.output_layer1=nn.Linear(3*hidden_dim,hidden_dim)\n",
        "    self.output_layer2=nn.Linear(hidden_dim,n_classes)\n",
        "    #Working variables of model\n",
        "    self.reasoning_matrix=nn.Parameter(torch.randn(self.p,hidden_dim,hidden_dim*2))\n",
        "\n",
        "    # Utility Params\n",
        "    self.FORWARD=0\n",
        "    self.BACKWARD=1\n",
        "    self.NDIRECTIONS=2\n",
        "    self.n_classes=n_classes\n",
        "    #Initial control and memory states\n",
        "    self.c0=nn.Parameter(torch.randn(hidden_dim))\n",
        "    self.m0=nn.Parameter(torch.randn(hidden_dim))\n",
        "    self.m=torch.zeros(self.p+1, hidden_dim)\n",
        "    self.c=torch.zeros(self.p+1, hidden_dim)\n",
        "    self.r=torch.zeros(self.p, hidden_dim)\n",
        "    self.wipe_memory()\n",
        "  \n",
        "  \n",
        "  def wipe_memory(self):\n",
        "    self.c=torch.zeros(self.p+1, self.hidden_dim)\n",
        "    self.m=torch.zeros(self.p+1, self.hidden_dim)\n",
        "    self.r=torch.zeros(self.p+1, self.hidden_dim)\n",
        "    self.q=torch.zeros(2*self.hidden_dim)\n",
        "    self.q_i=torch.zeros(self.p,self.hidden_dim)\n",
        "    self.c[0]=self.c0.clone()\n",
        "    self.m[0]=self.m0.clone()\n",
        "    \n",
        "  def forward(self, input, image):\n",
        "    self.contextual_words=None #We clear last input\n",
        "    self.input_step(input, image)\n",
        "    self.mac_step()\n",
        "  \n",
        "    return self.output_step()\n",
        "  \n",
        "  def input_step(self,input, image):\n",
        "    image=image.view(1,3,224,224)\n",
        "    question_result=self.question_processing(input)\n",
        "    self.knowledge_base=self.get_knowledge_base(image)\n",
        "    \n",
        "    return question_result, self.knowledge_base\n",
        "  \n",
        "  def question_processing(self,input):\n",
        "    result, (h_n,c_n)=self.inputLSTM(input)\n",
        "    #We get Contextual Words from LSTM Output\n",
        "    seq_length=len(input)\n",
        "    result=result.view(seq_length, 1, self.NDIRECTIONS, self.hidden_dim)\n",
        "    self.contextual_words=result[:,0,self.FORWARD,:]\n",
        "    h_n=h_n.view(1, 2, 1, self.hidden_dim)\n",
        "    cw_b=h_n[0][self.BACKWARD][0]\n",
        "    cw_f=h_n[0][self.FORWARD][0]\n",
        "    #Calculate q_i\n",
        "    self.q=torch.cat((cw_b,cw_f),0).view(2*self.hidden_dim,1)\n",
        "    \n",
        "    for i in range(self.p):\n",
        "      candidate=torch.mm(self.reasoning_matrix[i], self.q)+self.q_bias[i].view(self.hidden_dim,1)\n",
        "      self.q_i[i]=candidate.view(self.hidden_dim)\n",
        "      \n",
        "    return self.q_i\n",
        "  \n",
        "  def get_knowledge_base(self, image):\n",
        "    result=self.image_processor(image)\n",
        "    return result\n",
        "  \n",
        "  def mac_step(self):\n",
        "    for r in range(self.p):\n",
        "      self.control_step(r)\n",
        "      self.read_step(r)\n",
        "      self.write_step(r)\n",
        "\n",
        "  def control_step(self, reasoning_step):\n",
        "      seq_length=len(self.contextual_words)\n",
        "      #c1\n",
        "      cq=torch.mm(self.W_CQ,torch.cat((self.c[reasoning_step].clone(),self.q_i[reasoning_step])).view(2*self.hidden_dim,1))\n",
        "      #c2.1\n",
        "      cq_M=cq.repeat(1,seq_length)\n",
        "      ca=torch.mm(self.W_CA, (cq_M.transpose(1,0)*self.contextual_words).transpose(1,0))+self.bias_ca[reasoning_step] \n",
        "      #c2.2\n",
        "      cv=nn.functional.softmax(ca.view(seq_length),dim=0)\n",
        "      #c2.3\n",
        "      self.c[reasoning_step+1]=torch.mm(cv.view(1,seq_length),self.contextual_words)\n",
        " \n",
        "  def read_step(self, reasoning_step):\n",
        "    \n",
        "    H=14\n",
        "    W=14\n",
        "    d=self.hidden_dim\n",
        "    \n",
        "    #r1\n",
        "    memory_vector=torch.mm(self.W_M,self.m[reasoning_step].clone().view(hidden_dim,1))+self.bias_m\n",
        "    memory_matrix=memory_vector.repeat(14,1,14).transpose(0,1)\n",
        "    kb_matrix = torch.mm(self.W_K,(self.knowledge_base[0]).view(d,H*W)+self.bias_k).view(d,H,W)\n",
        "    I_hw=memory_matrix*kb_matrix\n",
        "    #r2\n",
        "    tempI=I_hw.view(d,H*W)\n",
        "    tempKB=self.knowledge_base[0].view(d,H*W)\n",
        "    I_hwp=(torch.mm(self.W_IHW,torch.cat((tempI,tempKB),dim=0))+self.bias_ihw).view(d,H,W)\n",
        "    #r3.1\n",
        "    temp=self.c[reasoning_step+1].clone().repeat(H,W,1).transpose(2,0)\n",
        "    ra=(torch.mm(self.W_R,(temp*I_hwp).view(d,H*W))+self.bias_ra).view(1,H,W)\n",
        "    #r3.2\n",
        "    rv=nn.functional.softmax(ra.view(H*W),dim=0).view(1,H,W)\n",
        "    #r3.3\n",
        "    #TODO: redo as matrix multiplication\n",
        "    weighted_vectors=torch.zeros(H,W,d)\n",
        "    result=torch.zeros(d)\n",
        "    for i in range(H):\n",
        "      for j in range(W):\n",
        "        result+=rv[:,i,j]*self.knowledge_base[0,:,i,j]\n",
        "    \n",
        "    self.r[reasoning_step]=result\n",
        "    \n",
        "    return self.r\n",
        "  def write_step(self, reasoning_step):\n",
        "    H=14\n",
        "    W=14\n",
        "    d=self.hidden_dim\n",
        "   \n",
        "    #w1\n",
        "    m_info=torch.mm(self.W_MI,torch.cat((self.r[reasoning_step].clone(),self.m[reasoning_step].clone())).view(2*d,1))+self.bias_mip\n",
        "    #w2.1\n",
        "    sa_ij=nn.functional.softmax(torch.mm(self.W_SA,(self.c[reasoning_step+1].clone()*self.c[0:reasoning_step+1].clone()).transpose(1,0)).view(reasoning_step+1,1)+self.bias_sa,dim=0)\n",
        "    #w2.2\n",
        "    mi_sa=torch.mm(sa_ij.view(1,reasoning_step+1),self.m[0:reasoning_step+1].clone()).transpose(1,0)\n",
        "    #w2.3\n",
        "    mi_prime=torch.mm(self.W_P,m_info)+self.bias_mi#+torch.mm(self.W_S,mi_sa)\n",
        "    #w3.1\n",
        "    c_ip=torch.mm(self.W_CI,self.c[reasoning_step].clone().view(d,1))+self.bias_ci\n",
        "    #w3.2\n",
        "    new_memory=torch.sigmoid(c_ip)*self.m[reasoning_step].clone()+(1-torch.sigmoid(c_ip))*mi_prime.view(d)\n",
        "    #I added this line to avoid memory values exploding over the reasoning process\n",
        "    new_memory=new_memory/new_memory.norm()\n",
        "    self.m[reasoning_step+1]=new_memory\n",
        "    return None\n",
        "  def output_step(self):\n",
        "    output=torch.cat((self.q.view(2*self.hidden_dim),self.m[self.p]),dim=0)\n",
        "    output=nn.functional.relu(self.output_layer1(output))\n",
        "    output=self.output_layer2(output)\n",
        "    output=nn.functional.softmax(output,dim=0)\n",
        "    return output\n",
        "    \n",
        "  #Code for extracting features from ResNet101\n",
        "  def load_resnet_model(self,d=100,pretrained=True):\n",
        "    class ResNetNoBottom(torch.nn.Module):\n",
        "      def __init__(self, original_model,d):\n",
        "        super(ResNetNoBottom, self).__init__()\n",
        "        self.features = torch.nn.Sequential(*list(original_model.children())[:-3])\n",
        "        for param in self.features.parameters():\n",
        "          param.requires_grad=False\n",
        "        self.convExtra1=nn.Conv2d(1024,d,1,1)\n",
        "        self.convExtra2=nn.Conv2d(d,d,1,1)\n",
        "\n",
        "      def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.convExtra1(x)\n",
        "        x = self.convExtra2(x)\n",
        "        return x\n",
        "\n",
        "    resnet101 = torchvision.models.resnet101(pretrained=pretrained)\n",
        "    return ResNetNoBottom(resnet101,d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kZSjl9nj0Kaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as opt\n",
        "import time\n",
        "def runModel(model,nEpochs,input_data,output_data,n_print=5):\n",
        "  n_print=1 #How many epochs until we write an update for the MSE\n",
        "  n_samples=len(input_data[0])\n",
        "  errors=list()\n",
        "  x=list()\n",
        "  print(\"Running Model: {}\".format(model.__class__.__name__))\n",
        "  optimizer = opt.Adam(model.parameters(), lr=0.1)\n",
        "  loss=nn.MSELoss()\n",
        "  for epoch in range(nEpochs):\n",
        "    print(\"Starting Epoch N°{}\".format(epoch+1))\n",
        "    running_loss = 0.0\n",
        "    sentences=input_data[0]\n",
        "    images=input_data[1]\n",
        "    for i,sample in enumerate(sentences):\n",
        "      model.zero_grad()\n",
        "      \n",
        "      model.wipe_memory()\n",
        "     \n",
        "      output=model(sample,images[i])\n",
        "      \n",
        "      result=loss(output,output_data[i])\n",
        "\n",
        "      result.backward(retain_graph=False)\n",
        "   \n",
        "      optimizer.step()\n",
        "      \n",
        "      running_loss += result.item()\n",
        "      '''print(\"R:\",model.r)\n",
        "      print(\"C:\",model.c)\n",
        "      print(\"M:\",model.m)\n",
        "      print(\"M0:\",model.m0)\n",
        "      print(\"C0:\",model.c0)'''\n",
        "    error=running_loss/n_samples\n",
        "    errors.append(error)\n",
        "    if  epoch % n_print == 0:\n",
        "      print(\"Average MSE: {} for Epoch {}\".format(error,epoch+1))\n",
        "  return model, errors #For plotting purposes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zI7hPVmJq56R",
        "colab_type": "code",
        "outputId": "f46cfd16-e6bf-44c5-c527-62bdf1aaff49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "embed_dim=10\n",
        "batch_size=1\n",
        "seq_length=5\n",
        "hidden_dim=10\n",
        "reasoning_steps=5\n",
        "samples=100\n",
        "n_classes=2\n",
        "mac_net=MACNet(embed_dim,hidden_dim, reasoning_steps,n_classes)\n",
        "\n",
        "input = torch.zeros(samples,seq_length,batch_size,embed_dim)\n",
        "import torchvision.transforms as transforms\n",
        "from skimage import io, transform\n",
        "import os\n",
        "import numpy as np\n",
        "gatos=\"cat.\"\n",
        "perros=\"dog.\"\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "data=list()\n",
        "output=list()\n",
        "for img in range(100):\n",
        "  image=io.imread(os.path.join('dogs_cats_sample/', gatos+str(img)+\".jpg\"))\n",
        "  image = torch.from_numpy(transform.resize(image, (224, 224))).float()\n",
        "\n",
        "  data.append(normalize(image/255))\n",
        "  output.append(torch.tensor([1,0]).float())\n",
        "  image=io.imread(os.path.join('dogs_cats_sample/', perros+str(img)+\".jpg\"))\n",
        "  image = torch.from_numpy(transform.resize(image, (224, 224))).float()\n",
        "  data.append(normalize(image/255))\n",
        "  output.append(torch.Tensor([0,1]).float())\n",
        "\n",
        "#image_input=torch.zeros(samples,3,224,224)\n",
        "#image_input.random_(0,255)\n",
        "output_data=torch.randn(samples,n_classes)\n",
        "for i,el in enumerate(output_data):\n",
        "  output_data[i]=nn.functional.softmax(el, dim=0)\n",
        "input_data=list()\n",
        "input_data.append(input)\n",
        "input_data.append(data)\n",
        "\n",
        "mac_net(input[0], data[0])\n",
        "\n",
        "#print(\"Control:\",result)\n",
        "#print(\"Memoria:\",image)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5125, 0.4875], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "-AES30ZH5uBt",
        "colab_type": "code",
        "outputId": "3346eb3b-13d4-424b-c993-9067a800011c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "model, errors=runModel(mac_net,10,input_data,output)\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Model: MACNet\n",
            "Starting Epoch N°1\n",
            "Average MSE: 0.255278826058 for Epoch 1\n",
            "Starting Epoch N°2\n",
            "Average MSE: 0.251421414614 for Epoch 2\n",
            "Starting Epoch N°3\n",
            "Average MSE: 0.251315880716 for Epoch 3\n",
            "Starting Epoch N°4\n",
            "Average MSE: 0.251316294968 for Epoch 4\n",
            "Starting Epoch N°5\n",
            "Average MSE: 0.251316610277 for Epoch 5\n",
            "Starting Epoch N°6\n",
            "Average MSE: 0.251316821575 for Epoch 6\n",
            "Starting Epoch N°7\n",
            "Average MSE: 0.251316961944 for Epoch 7\n",
            "Starting Epoch N°8\n",
            "Average MSE: 0.251317068338 for Epoch 8\n",
            "Starting Epoch N°9\n",
            "Average MSE: 0.251317140758 for Epoch 9\n",
            "Starting Epoch N°10\n",
            "Average MSE: 0.251317205429 for Epoch 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U5Qh4T6h4xjF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(name, param)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1zqrkeNGitL3",
        "colab_type": "code",
        "outputId": "f3cf3f6e-f55d-423b-ac27-c2fc5671a84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"MEMORY,\",model.m)\n",
        "print(\"READ,\",model.r)\n",
        "print(\"CONTROL,\",model.c)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('MEMORY,', tensor([[ -1.7046,  -0.8735,   0.1411,  -1.4196,   1.4526,  -0.3175,   0.0305,\n",
            "          -0.6452,   0.0056,   1.5256],\n",
            "        [ -2.1049,  -0.1052,   0.7353,  -1.1375,   1.7160,  -0.3764,   0.4080,\n",
            "          -0.6157,  -0.1103,   1.0577],\n",
            "        [ -9.6281,  14.3336,  11.9030,   4.1641,   6.6665,  -1.4816,   7.5037,\n",
            "          -0.0612,  -2.2866,  -7.7356],\n",
            "        [ -8.4134,  11.2923,   9.9382,   6.2258,   4.1051,   1.2810,  -0.2851,\n",
            "           2.6954,   0.2859,  -9.0008],\n",
            "        [ -7.2000,   8.2498,   7.9734,   8.2842,   1.5438,   4.0429,  -8.0737,\n",
            "           5.4545,   2.8528, -10.2639],\n",
            "        [ -5.9878,   5.2062,   6.0086,  10.3392,  -1.0173,   6.8038, -15.8620,\n",
            "           8.2162,   5.4141, -11.5249]], grad_fn=<CopySlices>))\n",
            "('READ,', tensor([[    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
            "             0.0000,     0.0000, 25900.0293,     0.0000],\n",
            "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
            "             0.0000,     0.0000, 25900.0293,     0.0000],\n",
            "        [    0.0000,     0.0000,     0.0000,  9750.8350,     0.0000,     0.0000,\n",
            "             0.0000,     0.0000,     0.0000,     0.0000],\n",
            "        [    0.0000,     0.0000,     0.0000,  9750.8350,     0.0000,     0.0000,\n",
            "             0.0000,     0.0000,     0.0000,     0.0000],\n",
            "        [    0.0000,     0.0000,     0.0000,  9750.8350,     0.0000,     0.0000,\n",
            "             0.0000,     0.0000,     0.0000,     0.0000],\n",
            "        [    0.0000,     0.0000,     0.0000,     0.0000,     0.0000,     0.0000,\n",
            "             0.0000,     0.0000,     0.0000,     0.0000]],\n",
            "       grad_fn=<CopySlices>))\n",
            "('CONTROL,', tensor([[-1.5733e+00, -4.7605e-01,  1.4508e+00,  4.8298e-01,  4.9062e-01,\n",
            "         -9.4647e-01, -8.2857e-01, -2.9204e-01, -9.4927e-01, -1.4610e+00],\n",
            "        [-9.1767e-01, -2.4869e-03,  8.8740e-01,  9.0839e-06,  9.9906e-01,\n",
            "         -7.9581e-06, -1.6408e-03,  9.9927e-01, -9.9940e-01, -9.9909e-01],\n",
            "        [-9.1756e-01, -2.4942e-03,  8.8723e-01,  9.1444e-06,  9.9899e-01,\n",
            "         -8.0022e-06, -1.6458e-03,  9.9921e-01, -9.9933e-01, -9.9903e-01],\n",
            "        [-9.1757e-01, -2.4938e-03,  8.8724e-01,  9.1407e-06,  9.9899e-01,\n",
            "         -7.9996e-06, -1.6455e-03,  9.9921e-01, -9.9933e-01, -9.9903e-01],\n",
            "        [-9.1759e-01, -2.4922e-03,  8.8728e-01,  9.1272e-06,  9.9901e-01,\n",
            "         -7.9898e-06, -1.6444e-03,  9.9923e-01, -9.9935e-01, -9.9905e-01],\n",
            "        [-9.1741e-01, -2.5048e-03,  8.8701e-01,  9.2403e-06,  9.9890e-01,\n",
            "         -8.0684e-06, -1.6529e-03,  9.9914e-01, -9.9924e-01, -9.9895e-01]],\n",
            "       grad_fn=<CopySlices>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BtP37jkr5BBQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "resultado=model_ft(random_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ruP__icA5Kii",
        "colab_type": "code",
        "outputId": "387198bf-0290-40fc-9fc6-12503a003eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "resultado.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 100, 14, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "metadata": {
        "id": "KyJ2_C9t4yvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model_ft)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RqE3UVD-8nJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "def convolution_size(in_features,kernel_size,stride,padding):\n",
        "  return math.floor((in_features+2*padding-kernel_size)/stride)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jY-owSTm9qpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "(layer3): Sequential(\n",
        "    (0): Bottleneck(\n",
        "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
        "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace)\n",
        "      (downsample): Sequential(\n",
        "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
        "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      )"
      ]
    },
    {
      "metadata": {
        "id": "6Hz81qMh9Ms8",
        "colab_type": "code",
        "outputId": "e27ef2bb-f1e6-4a17-92fb-9404c53718cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "convolution_size(224,7,2,3)\n",
        "convolution_size(112,3,2,1)\n",
        "convolution_size(56,1,1,0)\n",
        "convolution_size(56,3,1,1)\n",
        "convolution_size(56,1,1,0)\n",
        "convolution_size(56,1,1,0)\n",
        "convolution_size(56,3,2,1)\n",
        "convolution_size(28,1,2,0)\n",
        "convolution_size(14,3,2,1)\n",
        "convolution_size(7,1,2,0)\n",
        "convolution_size(4,3,2,1)\n",
        "convolution_size(2,1,2,0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "YV3m15AAp6Hl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "lbCyOh_xC9G2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for name, param in model_ft.named_parameters():\n",
        "  print(name, param.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ml_44tuUpB02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "954538fd-d51c-45a2-f4a8-1ad2380ac5d1"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cb4mcKMCyyY8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        },
        "outputId": "866c9f5b-72a4-496b-d106-5a125baa252c"
      },
      "cell_type": "code",
      "source": [
        "print(data[0])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.0031, 0.0025, 0.0013],\n",
            "         [0.0032, 0.0026, 0.0014],\n",
            "         [0.0032, 0.0026, 0.0014],\n",
            "         ...,\n",
            "         [0.0038, 0.0031, 0.0018],\n",
            "         [0.0037, 0.0031, 0.0019],\n",
            "         [0.0037, 0.0031, 0.0019]],\n",
            "\n",
            "        [[0.0031, 0.0025, 0.0013],\n",
            "         [0.0032, 0.0026, 0.0014],\n",
            "         [0.0032, 0.0026, 0.0014],\n",
            "         ...,\n",
            "         [0.0038, 0.0032, 0.0018],\n",
            "         [0.0037, 0.0031, 0.0019],\n",
            "         [0.0037, 0.0031, 0.0019]],\n",
            "\n",
            "        [[0.0031, 0.0025, 0.0013],\n",
            "         [0.0032, 0.0026, 0.0014],\n",
            "         [0.0032, 0.0026, 0.0014],\n",
            "         ...,\n",
            "         [0.0038, 0.0031, 0.0019],\n",
            "         [0.0037, 0.0031, 0.0019],\n",
            "         [0.0037, 0.0031, 0.0019]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0024, 0.0019, 0.0009],\n",
            "         [0.0024, 0.0019, 0.0009],\n",
            "         [0.0024, 0.0019, 0.0009],\n",
            "         ...,\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.0024, 0.0019, 0.0008],\n",
            "         [0.0024, 0.0019, 0.0008],\n",
            "         [0.0024, 0.0019, 0.0009],\n",
            "         ...,\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000]],\n",
            "\n",
            "        [[0.0023, 0.0019, 0.0008],\n",
            "         [0.0023, 0.0019, 0.0008],\n",
            "         [0.0024, 0.0019, 0.0008],\n",
            "         ...,\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "siomKBjFxhr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "150aee2d-2eb5-4bfe-f746-32375f3a3536"
      },
      "cell_type": "code",
      "source": [
        "!ls data/data"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'data/data': Not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}